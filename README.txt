This script performs a link check on a input list of urls and determine which urls have a broken domain(no domain registered).

It is based on scrapy(http://scrapy.org/) and it run via scrapyd(https://scrapyd.readthedocs.org/en/latest/).


========
Settings
========

Script settings - You can easily configure the crawler by changing lots of settings(check this list of all possible settings http://doc.scrapy.org/en/latest/topics/settings.html).

Currently there are 3 settings file:
    - base.py - here we set up the basic settings for the crawler.
    - local.py - settings for local development.
    - prod.py  - settings for production running.

Most of the settings are self explanatory so that are easy to understand it's usage.


==========
Deployment
==========

Currently there is a deploy script which help us to deploy the app and also perform tasks like: setup server, deploy crawler, run spider.
Deployment tool is done via fabric(http://www.fabfile.org/). To run a task simple use the following command form:

    fab task_name

Available commands:

    copy_to_server        Sync files from local to server
    deploy                Most important: perform:copy_to_server, deploy_scrapy_config, scrapyd_deploy and schedule
    deploy_scrapy_config  Make sure that on server we have the set the prod settings
    install_requirements  Install specific python libraries used by the crawler
    schedule              Schedule the spider based on spider_id set in ./schedule file
    scrapyd_deploy        Deploy spider to scrapyd(in case that you made any changes)
    scrapyd_restart       Restart scrapyd service
    setup                 Setup all server dependencies so that crawler can run
    supervisor_setup      Setup supervisor which is used to run scrapyd

Most of time when you do a change in spider you need to run the command: fab deploy.

==============
Running spider
==============

You can run a spider using the REST API of the scrapyd service. Here is an example running the spider using curl command:

    curl http://52.25.64.223:6800/schedule.json -d project=linkchecker -d spider=linkcheck -d spider_id=2_1435487805_dXfUm

most important to note here is the 'spider_id=2_1435487805_dXfUm' - this is the id of the spider generated by the web app
and used to get the link: http://52.11.110.28/data/projects/2_1435378586_yNDYd/google_scraper.out.

Also note here the settings we pass to spider: setting=DEPTH_LIMIT=2. We can pass any settings that is allowed by the scrapy here.
If we want a different depth limit we can simple set it: -d setting=DEPTH_LIMIT=3.


